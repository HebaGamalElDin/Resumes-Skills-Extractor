{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ULQnzrLQyJw"
   },
   "outputs": [],
   "source": [
    "###############\n",
    "## Libraries ##\n",
    "###############\n",
    "import pandas as pd\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import nltk\n",
    "from itertools import chain\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.en import parsetree\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SsGxveqBQyKE",
    "outputId": "6bd01067-fb74-45bf-bf3b-dd5e680f6b6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################\n",
    "## Readind A Sample ##\n",
    "######################\n",
    "DF = pd.read_csv('sample_jobs_data.csv').drop_duplicates().drop(['title'], axis=1).drop(['jobFunction'], axis=1).drop(['industry'], axis=1)\n",
    "DF = DF[(DF.skills != \"['nan']\") & (DF.description != \"NaN\")]\n",
    "DF = DF.iloc[:1000]\n",
    "DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvpPB1LMQyKX"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "## Preparing Data ##\n",
    "###################\n",
    "\n",
    "#Description Column\n",
    "DF.description = DF.description.str.lower().str.replace('[^\\w\\s]', '')\n",
    "stop = stopwords.words('english')\n",
    "DF.description = DF.description.astype(str)\n",
    "DF.description = DF.description.apply(lambda x: \" \".join((x for x in x.split() if x not in stop)))\n",
    "\n",
    "#Requirements Column\n",
    "DF.requirements = DF.requirements.str.lower()\n",
    "DF.requirements = DF.requirements.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "DF.requirements = DF.requirements.apply(ast.literal_eval)\n",
    "\n",
    "#Skills Column\n",
    "DF.skills = DF.skills.str.replace(r'[^\\w\\s,[ ] ]' , '').str.lower()\n",
    "DF.skills = DF.skills.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "DF.skills = DF.skills.apply(ast.literal_eval)\n",
    "Skills = list(set(chain(*DF.skills)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlw7G1XiQyKp"
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "## POS Tagging & Chunking ##\n",
    "############################\n",
    "def Chunking_(DESC):\n",
    "    chunks = {}\n",
    "    Tokenized_ = {}\n",
    "    for index, sent in enumerate(DESC):\n",
    "        chunks[index] = parsetree(sent, relations=True, lemmata=True, encoding = 'utf-8',tokenize = False,tags = True,chunks = True,tagset='universal')\n",
    "        Tokenized_[index] = word_tokenize(sent)\n",
    "    return  chunks, Tokenized_\n",
    "#######################\n",
    "## Get Noun Phrases ##\n",
    "#####################\n",
    "def Get_Nouns(chunks):\n",
    "    NPs = {}\n",
    "    for Key,Val in chunks.items():\n",
    "        for sentence in Val:\n",
    "            NPs[Key] = [word.string for chunk in sentence.chunks if chunk.type == 'NP' for word in chunk.words  if (word.type == 'NN') or (word.type == 'NNS') or (word.type == 'NNP')]\n",
    "    return NPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTQjXYzKQyKz"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "## Matching Skills with NPs ##\n",
    "#############################\n",
    "def Match_skill(Nouns):\n",
    "    Candidates_ = {}\n",
    "    for Ind, Cand_ in Nouns.items():\n",
    "        Candidates_[Ind] = [(word.strip(),1) if word in Skills else (word.strip(),0) for word in Cand_]\n",
    "    return Candidates_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmvS8UJ0QyLE"
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "## Get Context & Candidate phrase with Context ##\n",
    "#################################################\n",
    "def Get_(Word, Index, Tokens):\n",
    "    Ind = Tokens[Index].index(Word)\n",
    "    Context = Tokens[Index][Ind-3:Ind] + Tokens[Index][Ind+1:Ind+4]\n",
    "    Combined_ = Tokens[Index][Ind-3:Ind] + [Word] + Tokens[Index][Ind+1:Ind+4]\n",
    "    return Context , Combined_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "03BBvqN4QyLO"
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "## Set The Final DataFrame ##\n",
    "############################\n",
    "def Set_(Candidates_, Tokens):\n",
    "    Final_DF = []\n",
    "    for Indx,Value in Candidates_.items():\n",
    "        for tup in Value:\n",
    "            Cont, Comb = Get_(tup[0], Indx, Tokens)\n",
    "            Final_DF.append([tup[0], Cont, Comb, tup[1]])\n",
    "    Final_DF_ = pd.DataFrame(list(Final_DF), columns= ['Candidate_Phrase', 'Context', 'Combined' , 'Class'])\n",
    "    Major = Final_DF_[Final_DF_.Class==1]\n",
    "    Minor = Final_DF_[Final_DF_.Class==0]\n",
    "    Minor_ = resample(Minor, replace=True, n_samples=Final_DF_.Class.value_counts()[1], random_state=42)\n",
    "    DF1_ = pd.concat([Major, Minor_])\n",
    "    Final_DF_ = pd.DataFrame(DF1_, columns= ['Candidate_Phrase', 'Context', 'Combined' , 'Class'])\n",
    "    Final_DF_ = Final_DF_.sample(frac=1).reset_index(drop=True)\n",
    "    return Final_DF_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-FAEEjUQyLi"
   },
   "outputs": [],
   "source": [
    "##################\n",
    "## The Pipeline ##\n",
    "##################\n",
    "def Pipeline_():\n",
    "    Chunks, Tokens = Chunking_(DF.description)    \n",
    "    Nouns = Get_Nouns(Chunks)\n",
    "    Candidates = Match_skill(Nouns)\n",
    "    DF_ = Set_(Candidates, Tokens)\n",
    "    print(\"DONE!\")\n",
    "    return DF_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FOJKv9fbQyLr",
    "outputId": "f3767dc2-9fdb-4d11-d8c8-5e49273e4c7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "## Calling The pipeline ##\n",
    "#########################\n",
    "DF_ = Pipeline_()\n",
    "DF_\n",
    "#DF_.Class.value_counts()\n",
    "DF_.to_csv(\"Final DF.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFqXTkcwQyL6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DF_ = pd.read_csv(\"/ColabNotebooks/My Drive/ColabNotebooks/Final DF.csv\" , index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ae0oifFrQyMN"
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "## Prepare Glove Pre-Trained Model ## \n",
    "#####################################\n",
    "def Prepare_GloveF_(Glove_File):\n",
    "    File = str(Glove_File)\n",
    "    embeddings_dict = {}\n",
    "    with open(File, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "Res = Prepare_GloveF_(r\"E:\\New Moonlit Stage\\iNetworks Intern\\Practical Task #6\\glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NqtpmoPpQyMa"
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "## Preparing Before Embeddings ##\n",
    "################################\n",
    "def Prepare_For_E(Words):\n",
    "    global Max_Words \n",
    "    Max_Words = 1000\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(Words)\n",
    "    Out = tokenizer.texts_to_sequences(Words)\n",
    "    vocab_size = tokenizer.word_index\n",
    "    Output = pad_sequences(Out, maxlen=Max_Seq)\n",
    "    return vocab_size, Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w3b4K_QjQyMr"
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "## Prepare Embedding Layer ##\n",
    "############################\n",
    "def Embeddings_(Column):\n",
    "    global Max_Seq, Emb_Dim\n",
    "    vocab_size = Prepare_For_E(Column)[0]\n",
    "    N_words = len(vocab_size) + 1\n",
    "    Max_Seq = 50\n",
    "    Emb_Dim = 50\n",
    "    embedding_matrix = np.zeros((N_words, Emb_Dim))\n",
    "    for word, index in vocab_size.items():\n",
    "        if index > Max_Words:\n",
    "            continue\n",
    "        embedding_vector = Res.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "    Layer = Embedding(N_words, Emb_Dim, embeddings_initializer=Constant(embedding_matrix), input_length=Max_Seq,trainable=False)\n",
    "    return Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dIB0vBcKQyM1"
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "## Get Every Layer being Ready ##\n",
    "################################\n",
    "Phrase_Layer = Embeddings_(list(DF_.Candidate_Phrase.str.strip()))\n",
    "Context_Layer = Embeddings_([ast.literal_eval(Row) for Row in DF_.Context])\n",
    "Combined_Layer = Embeddings_([ast.literal_eval(Row) for Row in DF_.Combined])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nz16qh1jQyNB"
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "## Building LSTM Model ##\n",
    "########################\n",
    "def Build_Model():\n",
    "    lstm_input_phrase = tensorflow.keras.layers.Input(shape=(Max_Seq,))\n",
    "    lstm_input_cont = tensorflow.keras.layers.Input(shape=(Max_Seq,))\n",
    "    dense_input = tensorflow.keras.layers.Input(shape=(Max_Seq,))\n",
    "\n",
    "    emb_phrase = Phrase_Layer(lstm_input_phrase)\n",
    "    lstm_emb_phrase = tensorflow.keras.layers.LSTM(256)(emb_phrase)\n",
    "    lstm_emb_phrase = tensorflow.keras.layers.Dense(128, activation='relu')(lstm_emb_phrase)\n",
    "\n",
    "    emb_cont = Context_Layer(lstm_input_cont)\n",
    "    lstm_emb_cont = tensorflow.keras.layers.LSTM(256)(emb_cont)\n",
    "    lstm_emb_cont = tensorflow.keras.layers.Dense(128, activation='relu')(lstm_emb_cont)\n",
    "\n",
    "    dense_emb  = Combined_Layer(dense_input)\n",
    "    dense_emb = tensorflow.keras.layers.Dense(512, activation='relu')(dense_input)\n",
    "    dense_emb = tensorflow.keras.layers.Dense(256, activation='relu')(dense_emb)\n",
    "\n",
    "    tensorflow.keras.layers.concatenate([lstm_emb_phrase, lstm_emb_cont, dense_emb])\n",
    "\n",
    "    x = tensorflow.keras.layers.concatenate([lstm_emb_phrase, lstm_emb_cont, dense_emb])\n",
    "    x = tensorflow.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tensorflow.keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = tensorflow.keras.layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "    main_output = tensorflow.keras.layers.Dense(2, activation='softplus')(x)\n",
    "\n",
    "    Model = tensorflow.keras.models.Model(inputs=[lstm_input_phrase, lstm_input_cont, dense_input], outputs=main_output)\n",
    "    Optimizer = tensorflow.keras.optimizers.Adam(lr=0.0001)\n",
    "    Model.compile(optimizer=Optimizer, loss='binary_crossentropy')\n",
    "    print(\"Model Has Been Created Successfully!\")\n",
    "    print(Model.summary())\n",
    "    return Model\n",
    "\n",
    "def OneHot_Transform(Y):\n",
    "    onehot_y = []\n",
    "    for numb in Y:\n",
    "        onehot_arr = np.zeros(2)\n",
    "        onehot_arr[numb] = 1\n",
    "        onehot_y.append(np.array(onehot_arr))\n",
    "    return np.array(onehot_y)\n",
    "\n",
    "def Fitting(x_lstm_phrase, x_lstm_context, x_dense, y, val_split=0.30, patience=5, max_epochs=1000, batch_size=32):\n",
    "        x_lstm_phrase_seq = Prepare_For_E(x_lstm_phrase)[1]\n",
    "        x_lstm_context_seq = Prepare_For_E(x_lstm_context)[1]\n",
    "        x_dense_seq=Prepare_For_E(x_dense)[1]\n",
    "        y_onehot=OneHot_Transform(y)\n",
    "        Model.fit([x_lstm_phrase_seq, x_lstm_context_seq, x_dense_seq] ,y_onehot,\n",
    "                   batch_size=batch_size,\n",
    "                   epochs = max_epochs ,\n",
    "                   validation_split=val_split,\n",
    "                   callbacks=[tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)])\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 988
    },
    "colab_type": "code",
    "id": "YUJWelCZQyNJ",
    "outputId": "c8b08e58-5a5a-46a1-d5a3-d09351f99175"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Has Been Created Successfully!\n",
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_103 (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_104 (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_61 (Embedding)        (None, 50, 50)       154300      input_103[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_62 (Embedding)        (None, 50, 50)       507800      input_104[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_105 (InputLayer)          [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_43 (LSTM)                  (None, 256)          314368      embedding_61[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_44 (LSTM)                  (None, 256)          314368      embedding_62[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_171 (Dense)               (None, 512)          26112       input_105[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_169 (Dense)               (None, 128)          32896       lstm_43[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_170 (Dense)               (None, 128)          32896       lstm_44[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_172 (Dense)               (None, 256)          131328      dense_171[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_43 (Concatenate)    (None, 512)          0           dense_169[0][0]                  \n",
      "                                                                 dense_170[0][0]                  \n",
      "                                                                 dense_172[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_173 (Dense)               (None, 128)          65664       concatenate_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_174 (Dense)               (None, 64)           8256        dense_173[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_175 (Dense)               (None, 32)           2080        dense_174[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_176 (Dense)               (None, 2)            66          dense_175[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,590,134\n",
      "Trainable params: 928,034\n",
      "Non-trainable params: 662,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n",
      "389/389 [==============================] - 6s 14ms/step - loss: 7.5791 - val_loss: 7.6218\n",
      "Epoch 2/1000\n",
      "389/389 [==============================] - 5s 13ms/step - loss: 7.6240 - val_loss: 7.6218\n",
      "Epoch 3/1000\n",
      "389/389 [==============================] - 5s 13ms/step - loss: 7.6240 - val_loss: 7.6218\n",
      "Epoch 4/1000\n",
      "389/389 [==============================] - 5s 13ms/step - loss: 7.6240 - val_loss: 7.6218\n",
      "Epoch 5/1000\n",
      "389/389 [==============================] - 5s 13ms/step - loss: 7.6240 - val_loss: 7.6218\n",
      "Epoch 6/1000\n",
      "389/389 [==============================] - 5s 13ms/step - loss: 7.6240 - val_loss: 7.6218\n"
     ]
    }
   ],
   "source": [
    "Model = Build_Model()\n",
    "Fitting(list(DF_.Candidate_Phrase.str.strip()), [ast.literal_eval(Row) for Row in DF_.Context], [ast.literal_eval(Row) for Row in DF_.Combined], list(DF_.Class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K-IDjhfCY2hE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Skills-Extractor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
